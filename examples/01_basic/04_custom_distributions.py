"""\nCustom Distribution Definitions\n================================\n\nCreate your own probability distributions by extending\nthe distfit-pro framework.\n\nUse Case:\n---------\nYou need a distribution not in the standard library,\nor want to add domain-specific constraints.\n\nAuthor: Ali Sadeghi Aghili\n\"\"\"\n\nimport numpy as np\nfrom scipy import stats\nfrom distfit_pro.core.base import ContinuousDistribution, DistributionInfo\n\nprint(\"‚ïê" * 70)\nprint(\"üîß CUSTOM DISTRIBUTION DEFINITIONS\")\nprint(\"‚ïê" * 70)\n\n\n# ============================================================================\n# EXAMPLE 1: Simple Custom Distribution (Truncated Normal)\n# ============================================================================\n\nprint(\"\\n" + \"‚îÄ" * 70)\nprint(\"Example 1: Truncated Normal (0-100 range)\\n\")\n\nclass TruncatedNormal(ContinuousDistribution):\n    \"\"\"\n    Normal distribution truncated to [0, 100].\n    Useful for scores, percentages, etc.\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        # Bounds\n        self.lower = 0\n        self.upper = 100\n        # Will hold scipy truncnorm object\n        self._scipy_dist = None\n    \n    @property\n    def info(self) -> DistributionInfo:\n        return DistributionInfo(\n            name='truncnorm',\n            scipy_name='truncnorm',\n            display_name='Truncated Normal',\n            description='Normal distribution bounded to [0, 100]',\n            parameters=['loc', 'scale'],\n            support='[0, 100]',\n            is_discrete=False,\n            has_shape_params=False\n        )\n    \n    def _fit_mle(self, data: np.ndarray, **kwargs):\n        \"\"\"Maximum likelihood estimation\"\"\"\n        # Ensure data in range\n        if np.any(data < self.lower) or np.any(data > self.upper):\n            raise ValueError(f\"Data must be in [{self.lower}, {self.upper}]\")\n        \n        # Use scipy's truncnorm\n        # Convert bounds to standardized form\n        loc_init = np.mean(data)\n        scale_init = np.std(data)\n        \n        a = (self.lower - loc_init) / scale_init\n        b = (self.upper - loc_init) / scale_init\n        \n        # Fit\n        _, loc, scale = stats.truncnorm.fit(data, a, b)\n        \n        self._params = {'loc': loc, 'scale': scale}\n        \n        # Create scipy distribution\n        a_final = (self.lower - loc) / scale\n        b_final = (self.upper - loc) / scale\n        self._scipy_dist = stats.truncnorm(a_final, b_final, loc=loc, scale=scale)\n    \n    def _fit_mom(self, data: np.ndarray, **kwargs):\n        \"\"\"Method of moments\"\"\"\n        # Simple: use sample moments\n        loc = np.mean(data)\n        scale = np.std(data)\n        \n        self._params = {'loc': loc, 'scale': scale}\n        \n        # Create scipy distribution\n        a = (self.lower - loc) / scale\n        b = (self.upper - loc) / scale\n        self._scipy_dist = stats.truncnorm(a, b, loc=loc, scale=scale)\n    \n    def _get_scipy_params(self) -> dict:\n        \"\"\"Convert params to scipy format\"\"\"\n        loc = self._params['loc']\n        scale = self._params['scale']\n        a = (self.lower - loc) / scale\n        b = (self.upper - loc) / scale\n        return {'a': a, 'b': b, 'loc': loc, 'scale': scale}\n\n\n# Test it\nprint(\"Creating test data (exam scores 0-100):\")\nscores = np.clip(np.random.normal(75, 12, 300), 0, 100)\nprint(f\"  n={len(scores)}, mean={scores.mean():.1f}, std={scores.std():.1f}\\n\")\n\n# Fit custom distribution\ndist = TruncatedNormal()\ndist.fit(scores)\n\nprint(\"Fitted Truncated Normal:\")\nprint(dist.summary())\n\nprint(\"\\n‚úì Custom distribution works!\")\nprint(f\"  P(score > 85): {(1 - dist.cdf(85))*100:.1f}%\")\n\n\n# ============================================================================\n# EXAMPLE 2: Mixture Distribution (Two Normals)\n# ============================================================================\n\nprint(\"\\n" + \"‚ïê" * 70)\nprint(\"Example 2: Gaussian Mixture (2 Components)\\n\")\n\nclass GaussianMixture2(ContinuousDistribution):\n    \"\"\"\n    Mixture of two normal distributions.\n    Good for bimodal data (two subpopulations).\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self._scipy_dist = None\n        \n    @property\n    def info(self) -> DistributionInfo:\n        return DistributionInfo(\n            name='gaussmix2',\n            scipy_name='custom',\n            display_name='Gaussian Mixture (2)',\n            description='Mixture of two normal distributions',\n            parameters=['w', 'mu1', 'sigma1', 'mu2', 'sigma2'],\n            support='(-inf, inf)',\n            is_discrete=False,\n            has_shape_params=True\n        )\n    \n    def _fit_mle(self, data: np.ndarray, **kwargs):\n        \"\"\"Simple EM algorithm (simplified for demo)\"\"\"\n        from scipy.stats import norm\n        \n        # Initialize with k-means-style split\n        median = np.median(data)\n        cluster1 = data[data < median]\n        cluster2 = data[data >= median]\n        \n        # Initial estimates\n        mu1 = np.mean(cluster1)\n        sigma1 = np.std(cluster1)\n        mu2 = np.mean(cluster2)\n        sigma2 = np.std(cluster2)\n        w = len(cluster1) / len(data)\n        \n        # Simple EM (few iterations for demo)\n        for _ in range(10):\n            # E-step: assign responsibilities\n            p1 = w * norm.pdf(data, mu1, sigma1)\n            p2 = (1-w) * norm.pdf(data, mu2, sigma2)\n            resp1 = p1 / (p1 + p2 + 1e-10)\n            resp2 = 1 - resp1\n            \n            # M-step: update parameters\n            w = np.mean(resp1)\n            mu1 = np.sum(resp1 * data) / np.sum(resp1)\n            mu2 = np.sum(resp2 * data) / np.sum(resp2)\n            sigma1 = np.sqrt(np.sum(resp1 * (data - mu1)**2) / np.sum(resp1))\n            sigma2 = np.sqrt(np.sum(resp2 * (data - mu2)**2) / np.sum(resp2))\n        \n        self._params = {\n            'w': w,\n            'mu1': mu1,\n            'sigma1': sigma1,\n            'mu2': mu2,\n            'sigma2': sigma2\n        }\n    \n    def _fit_mom(self, data: np.ndarray, **kwargs):\n        \"\"\"Fallback to MLE\"\"\"\n        self._fit_mle(data, **kwargs)\n    \n    def _get_scipy_params(self) -> dict:\n        \"\"\"Not using scipy here\"\"\"\n        return {}\n    \n    def pdf(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Custom PDF calculation\"\"\"\n        from scipy.stats import norm\n        x = np.asarray(x)\n        p = self._params\n        pdf1 = norm.pdf(x, p['mu1'], p['sigma1'])\n        pdf2 = norm.pdf(x, p['mu2'], p['sigma2'])\n        return p['w'] * pdf1 + (1 - p['w']) * pdf2\n    \n    def cdf(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"Custom CDF calculation\"\"\"\n        from scipy.stats import norm\n        x = np.asarray(x)\n        p = self._params\n        cdf1 = norm.cdf(x, p['mu1'], p['sigma1'])\n        cdf2 = norm.cdf(x, p['mu2'], p['sigma2'])\n        return p['w'] * cdf1 + (1 - p['w']) * cdf2\n    \n    def mean(self) -> float:\n        \"\"\"Weighted mean\"\"\"\n        p = self._params\n        return p['w'] * p['mu1'] + (1 - p['w']) * p['mu2']\n\n\n# Test with bimodal data\nprint(\"Creating bimodal data (two customer segments):\")\n# Segment 1: Low spenders\nseg1 = np.random.normal(20, 5, 400)\n# Segment 2: High spenders\nseg2 = np.random.normal(80, 10, 100)\nspending = np.concatenate([seg1, seg2])\n\nprint(f\"  Total customers: {len(spending)}\")\nprint(f\"  Overall mean: ${spending.mean():.2f}\\n\")\n\n# Fit mixture\nmix = GaussianMixture2()\nmix.fit(spending)\n\nprint(\"Fitted Mixture:\")\nfor param, value in mix.params.items():\n    print(f\"  {param:10s} = {value:8.3f}\")\n\nprint(f\"\\n‚úì Discovered two segments!\")\nprint(f\"  Segment 1: {mix.params['w']*100:.1f}% at ${mix.params['mu1']:.2f}\")\nprint(f\"  Segment 2: {(1-mix.params['w'])*100:.1f}% at ${mix.params['mu2']:.2f}\")\n\n\n# ============================================================================\n# EXAMPLE 3: Distribution with Constraints\n# ============================================================================\n\nprint(\"\\n" + \"‚ïê" * 70)\nprint(\"Example 3: Constrained Gamma (shape >= 1)\\n\")\n\nclass ConstrainedGamma(ContinuousDistribution):\n    \"\"\"\n    Gamma distribution with shape >= 1.\n    Ensures unimodal distribution (mode at x > 0).\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self._scipy_dist = stats.gamma\n    \n    @property\n    def info(self) -> DistributionInfo:\n        return DistributionInfo(\n            name='gamma_constrained',\n            scipy_name='gamma',\n            display_name='Gamma (shape‚â•1)',\n            description='Gamma with unimodal constraint',\n            parameters=['alpha', 'beta'],\n            support='(0, inf)',\n            is_discrete=False,\n            has_shape_params=True\n        )\n    \n    def _fit_mle(self, data: np.ndarray, **kwargs):\n        \"\"\"MLE with constraint\"\"\"\n        # Standard gamma fit\n        shape, loc, scale = stats.gamma.fit(data, floc=0)\n        \n        # Enforce constraint\n        shape = max(shape, 1.0)\n        \n        self._params = {'alpha': shape, 'beta': scale}\n    \n    def _fit_mom(self, data: np.ndarray, **kwargs):\n        \"\"\"Method of moments with constraint\"\"\"\n        mean = np.mean(data)\n        var = np.var(data)\n        \n        # MoM estimates\n        scale = var / mean\n        shape = mean / scale\n        \n        # Enforce constraint\n        shape = max(shape, 1.0)\n        scale = mean / shape  # Adjust scale\n        \n        self._params = {'alpha': shape, 'beta': scale}\n    \n    def _get_scipy_params(self) -> dict:\n        return {'a': self._params['alpha'], 'scale': self._params['beta']}\n\n\nprint(\"Testing constrained Gamma:\")\ndata = np.random.gamma(2.5, 3, 300)\n\ndist = ConstrainedGamma()\ndist.fit(data)\n\nprint(f\"\\n  Shape (Œ±): {dist.params['alpha']:.3f}  (guaranteed ‚â• 1.0)\")\nprint(f\"  Scale (Œ≤): {dist.params['beta']:.3f}\")\nprint(f\"\\n‚úì Constraint enforced!\")\n\n\n# ============================================================================\n# KEY TAKEAWAYS\n# ============================================================================\n\nprint(\"\\n" + \"‚ïê" * 70)\nprint(\"üéì KEY TAKEAWAYS\")\nprint(\"‚ïê" * 70)\nprint(\"\"\"\n1. CUSTOM DISTRIBUTION STRUCTURE:\n   - Inherit from ContinuousDistribution or DiscreteDistribution\n   - Implement: info, _fit_mle(), _fit_mom(), _get_scipy_params()\n   - Optionally override: pdf(), cdf(), mean(), etc.\n\n2. USE CASES FOR CUSTOM DISTRIBUTIONS:\n   - Truncated/bounded versions of standard distributions\n   - Mixture models (multiple subpopulations)\n   - Domain-specific constraints (physics, business rules)\n   - Distributions not in scipy\n\n3. TIPS:\n   - Leverage scipy for standard components\n   - Keep _fit_mle() robust (handle edge cases)\n   - Provide good initial estimates for optimization\n   - Test with synthetic data first\n\n4. WHEN NOT TO CUSTOMIZE:\n   - Standard distribution already exists ‚Üí use it!\n   - Need quick solution ‚Üí use DistFitter with standard set\n   - Complexity not justified ‚Üí try simpler alternatives first\n\n5. NEXT STEPS:\n   - See scipy.stats documentation for distribution templates\n   - See examples/02_advanced/ for mixture models\n   - Consider contributing useful distributions back to distfit-pro!\n\"\"\")\