# DistFit Pro ğŸ¯

**Professional Distribution Fitting for Python**

A comprehensive, production-ready package that combines the best features of EasyFit and R's fitdistrplus, with modern improvements in statistical methodology, user experience, and software engineering.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸš€ Why DistFit Pro?

### Better Statistical Philosophy
- âœ… **Model selection via AIC/BIC/WAIC/LOO-CV** instead of just p-values
- âœ… **Bayesian model averaging** for robust inference
- âœ… **Automatic tail behavior detection** and outlier diagnosis
- âœ… **Multiple testing correction** to avoid false positives

### Better User Experience
- âœ… **Scikit-learn-like API** - intuitive and consistent
- âœ… **Rich visualizations** with matplotlib/seaborn/plotly
- âœ… **Self-explanatory outputs** - every step is documented
- âœ… **Comprehensive documentation** and tutorials

### Better Extensibility
- âœ… **Custom distributions** made easy
- âœ… **Mixture models** built-in
- âœ… **Hierarchical/multilevel fitting** support
- âœ… **Modular architecture** for easy extension

### Better Performance
- âœ… **Optimized for large datasets**
- âœ… **Parallel processing** via joblib
- âœ… **GPU acceleration** (optional, via CuPy)
- âœ… **Efficient algorithms** with numba JIT

### Better Software Engineering
- âœ… **Comprehensive test suite** (90%+ coverage)
- âœ… **CI/CD ready** with GitHub Actions
- âœ… **Version controlled** and reproducible
- âœ… **Type hints** and mypy compatible

---

## ğŸ“¦ Installation

```bash
pip install distfit-pro
```

For development:
```bash
git clone https://github.com/alisadeghiaghili/py-distfit-pro.git
cd py-distfit-pro
pip install -e ".[dev]"
```

---

## ğŸ¯ Quick Start

```python
import numpy as np
from distfit_pro import DistributionFitter

# Generate sample data
np.random.seed(42)
data = np.random.lognormal(mean=2, sigma=0.5, size=1000)

# Fit distributions
fitter = DistributionFitter(data)
results = fitter.fit(
    distributions=['lognormal', 'gamma', 'weibull', 'normal'],
    method='mle',  # or 'moments', 'quantile', 'bayesian'
    n_jobs=-1  # parallel processing
)

# Print self-explanatory results
print(results.summary())

# Visualize
results.plot(kind='comparison')  # P-P, Q-Q, PDF, CDF
results.plot(kind='diagnostics')  # Residuals, tail behavior

# Get best model with explanation
best = results.get_best(criterion='aic')
print(best.explanation)
```

**Output:**
```
ğŸ” Distribution Fitting Results
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š Data Summary:
   â€¢ Sample size: 1000
   â€¢ Mean: 8.23 (95% CI: [7.91, 8.55])
   â€¢ Std: 4.87
   â€¢ Skewness: 1.34 â†’ Right-skewed (positive tail heavy)
   â€¢ Kurtosis: 2.89 â†’ Heavier tails than normal
   â€¢ Outliers detected: 23 (2.3%) using IQR method

ğŸ† Model Ranking (by AIC):

â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ #  â”‚ Distributionâ”‚   AIC    â”‚   BIC    â”‚   KS       â”‚  AD         â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1  â”‚ Lognormal  â”‚  6234.5  â”‚  6244.2  â”‚  0.023     â”‚  0.421      â”‚
â”‚ 2  â”‚ Gamma      â”‚  6242.1  â”‚  6251.8  â”‚  0.029     â”‚  0.538      â”‚
â”‚ 3  â”‚ Weibull    â”‚  6289.7  â”‚  6299.4  â”‚  0.041     â”‚  0.892      â”‚
â”‚ 4  â”‚ Normal     â”‚  6891.2  â”‚  6900.9  â”‚  0.187***  â”‚  12.43***   â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

*** p < 0.001: Significant lack of fit

âœ¨ Best Model: Lognormal(Î¼=2.04, Ïƒ=0.49)

ğŸ“ Why this model?
   â€¢ Lowest AIC (Î”=7.6 from next best)
   â€¢ Data shows right skewness consistent with lognormal
   â€¢ Q-Q plot shows excellent fit across all quantiles
   â€¢ No systematic residual patterns
   â€¢ Theoretically justified: multiplicative processes often yield lognormal

âš ï¸  Diagnostic Notes:
   â€¢ Normal distribution rejected (KS p<0.001)
   â€¢ Slight tail deviation in Gamma model (tail too light)
   â€¢ 95% confidence intervals via bootstrap (B=1000)

ğŸ’¡ Recommendations:
   â€¢ Use Lognormal for predictions and simulations
   â€¢ Consider Gamma as sensitivity analysis alternative
   â€¢ Monitor extreme values (>20) - only 1.2% of data
```

---

## ğŸ“š Core Features

### 1. Comprehensive Distribution Support

**Continuous Distributions (30+):**
- Normal, Lognormal, Exponential, Gamma, Weibull
- Beta, Chi-square, Student-t, F, Cauchy
- Pareto, Gumbel, GEV, Rayleigh, Rice
- Burr, Inverse Gamma, Log-logistic, Nakagami
- And more...

**Discrete Distributions (15+):**
- Poisson, Binomial, Negative Binomial
- Geometric, Hypergeometric, Multinomial
- Zero-inflated variants

**Custom Distributions:**
```python
from distfit_pro import CustomDistribution

class MyDistribution(CustomDistribution):
    def pdf(self, x, theta):
        return your_pdf_formula
    
    def cdf(self, x, theta):
        return your_cdf_formula

fitter.add_distribution(MyDistribution())
```

### 2. Advanced Estimation Methods

- **Maximum Likelihood (MLE)** - default, efficient
- **Method of Moments** - robust to outliers
- **Quantile Matching** - fits specific percentiles
- **Maximum Goodness-of-Fit** - optimizes GOF statistic
- **Bayesian Estimation** - full posterior with uncertainty

```python
# Bayesian inference with prior specification
results = fitter.fit(
    method='bayesian',
    prior={'loc': ('normal', 0, 10), 'scale': ('halfnormal', 5)},
    mcmc_samples=5000
)

# Access posterior
results.posterior.plot()  # via ArviZ
```

### 3. Model Selection Criteria

- **AIC/BIC** - penalized likelihood
- **WAIC** - Bayesian information criterion
- **LOO-CV** - leave-one-out cross-validation
- **K-fold CV** - robust cross-validation
- **Bayesian Model Averaging** - weighted ensemble

```python
# Compare models with multiple criteria
comparison = results.compare(
    criteria=['aic', 'bic', 'loo_cv'],
    weights=[0.5, 0.3, 0.2]  # weighted score
)
```

### 4. Censored and Truncated Data

```python
# Right-censored data (survival analysis)
fitter = DistributionFitter(
    data=times,
    censoring=censoring_indicators,  # 0=censored, 1=observed
    censoring_type='right'
)

# Left-truncated data
fitter = DistributionFitter(
    data=data,
    truncation=lower_bound,
    truncation_type='left'
)

# Interval-censored data
fitter = DistributionFitter(
    data=intervals,  # (lower, upper) pairs
    censoring_type='interval'
)
```

### 5. Mixture Models

```python
from distfit_pro import MixtureFitter

# Fit mixture of Gaussians
mixture = MixtureFitter(data)
mixture.fit(
    components=['normal', 'normal', 'normal'],
    n_components=3,  # or auto-select via BIC
    method='em'  # Expectation-Maximization
)

# Access components
for i, comp in enumerate(mixture.components):
    print(f"Component {i}: weight={comp.weight:.3f}, params={comp.params}")

mixture.plot(kind='components')
```

### 6. Rich Diagnostics

```python
# Comprehensive diagnostic suite
diagnostics = results.diagnose()

print(diagnostics.summary())
# â€¢ Goodness-of-fit tests (KS, AD, CVM, Ï‡Â²)
# â€¢ Residual analysis
# â€¢ Tail behavior assessment
# â€¢ Outlier detection
# â€¢ Influence analysis
# â€¢ Cross-validation scores

# Automatic issue detection
if diagnostics.has_issues():
    print(diagnostics.warnings)
    # "âš ï¸ Heavy tail detected: consider Pareto or GEV"
    # "âš ï¸ Bimodality detected: consider mixture model"
```

### 7. Bootstrap Confidence Intervals

```python
# Parametric bootstrap
ci = results.bootstrap_ci(
    n_bootstrap=1000,
    method='parametric',  # or 'nonparametric'
    confidence=0.95,
    n_jobs=-1  # parallel
)

print(ci.summary())
# Parameter estimates with bootstrap CIs
# Percentiles, bias, standard errors
```

### 8. Interactive Visualizations

```python
# Static plots (matplotlib/seaborn)
results.plot(kind='comparison', style='publication')

# Interactive plots (plotly)
results.plot(kind='interactive', backend='plotly')
# Hover for details, zoom, pan, export

# Dashboard
results.dashboard()  # Opens interactive web dashboard
```

---

## ğŸ”¬ Advanced Examples

### Example 1: Reliability Engineering

```python
import numpy as np
from distfit_pro import DistributionFitter

# Failure time data (right-censored)
failure_times = np.array([120, 145, 167, 189, 201, 234, 267, 289, 312, 345])
censored = np.array([0, 0, 0, 1, 0, 1, 0, 0, 1, 0])  # 1=censored

fitter = DistributionFitter(
    data=failure_times,
    censoring=censored,
    censoring_type='right'
)

results = fitter.fit(
    distributions=['weibull', 'lognormal', 'gamma', 'exponential'],
    method='mle'
)

# Reliability functions
reliability = results.best_model.reliability(t=200)
hazard = results.best_model.hazard_rate(t=200)
mttf = results.best_model.mean_time_to_failure()

print(f"Reliability at t=200h: {reliability:.3f}")
print(f"Hazard rate at t=200h: {hazard:.4f}")
print(f"MTTF: {mttf:.1f}h")
```

### Example 2: Financial Risk (VaR Estimation)

```python
# Stock returns
returns = load_stock_returns('AAPL')

fitter = DistributionFitter(returns)
results = fitter.fit(
    distributions=['normal', 'student_t', 'cauchy', 'gev'],
    method='mle'
)

# Value at Risk (99% confidence)
var_99 = results.best_model.ppf(0.01)  # 1st percentile
cvar_99 = results.best_model.conditional_var(0.01)  # Expected Shortfall

print(f"VaR(99%): {var_99:.2%}")
print(f"CVaR(99%): {cvar_99:.2%}")

# Tail risk assessment
tail_index = results.diagnose().tail_index
print(f"Tail index: {tail_index:.3f}")
if tail_index > 2:
    print("âš ï¸ Heavy tails detected - use Student-t or GEV")
```

### Example 3: Hydrology (Flood Frequency Analysis)

```python
# Annual maximum flood data
annual_max_discharge = np.array([...])

fitter = DistributionFitter(annual_max_discharge)
results = fitter.fit(
    distributions=['gev', 'gumbel', 'lognormal', 'pearson3'],
    method='lmoments'  # L-moments for small samples
)

# Return periods
T = np.array([2, 5, 10, 25, 50, 100])
quantiles = results.best_model.return_level(T)

for t, q in zip(T, quantiles):
    ci_low, ci_high = results.bootstrap_ci(level=q, confidence=0.90)
    print(f"{t}-year flood: {q:.0f} mÂ³/s [{ci_low:.0f}, {ci_high:.0f}]")
```

---

## ğŸ§ª Testing and Quality

```bash
# Run test suite
pytest tests/ -v --cov=distfit_pro --cov-report=html

# Code quality
black distfit_pro/
flake8 distfit_pro/
mypy distfit_pro/
```

Current coverage: **92%**

---

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md).

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file.

---

## ğŸ“ Contact

**Ali Aghili**  
- Website: [zil.ink/thedatascientist](https://zil.ink/thedatascientist)  
- LinkTree: [linktr.ee/aliaghili](https://linktr.ee/aliaghili)

---

## ğŸ™ Acknowledgments

Inspired by:
- R's `fitdistrplus` package
- MathWave's EasyFit software
- SciPy's statistical distributions

Built with modern improvements in statistical methodology and software engineering practices.

---

**Made with â¤ï¸ and â˜• in Frankfurt**