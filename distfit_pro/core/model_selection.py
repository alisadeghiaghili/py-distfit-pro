"""Model Selection Criteria
========================

Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:
- AIC (Akaike Information Criterion)
- BIC (Bayesian Information Criterion)
- WAIC (Watanabe-Akaike Information Criterion)
- LOO-CV (Leave-One-Out Cross-Validation)

Ù‡Ø± Ù…Ø¹ÛŒØ§Ø± ØªÙˆØ¶ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ú†Ø±Ø§ ÛŒÚ© Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ù‡ØªØ± Ù…ÛŒâ€ŒØ¯Ø§Ù†Ø¯.
"""

from dataclasses import dataclass
from typing import List, Dict, Optional
import numpy as np
from scipy import stats


@dataclass
class ModelScore:
    """
    Ø§Ù…ØªÛŒØ§Ø² ÛŒÚ© Ù…Ø¯Ù„ Ø¨Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª
    """
    distribution_name: str
    criterion: str
    score: float
    n_params: int
    sample_size: int
    explanation: str
    rank: Optional[int] = None
    
    def __repr__(self) -> str:
        return f"{self.distribution_name}: {self.criterion}={self.score:.2f} (rank {self.rank})"


class ModelSelection:
    """
    Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„
    
    Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    
    @staticmethod
    def compute_aic(log_likelihood: float, n_params: int) -> float:
        """
        Akaike Information Criterion (AIC)
        
        ÙØ±Ù…ÙˆÙ„: AIC = 2k - 2ln(L)
        
        ØªÙˆØ¶ÛŒØ­:
        -------
        - k: ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„
        - L: likelihood
        - Ù…Ø¯Ù„ Ø¨Ø§ AIC Ú©Ù…ØªØ± Ø¨Ù‡ØªØ± Ø§Ø³Øª
        - Ø¬Ø±ÛŒÙ…Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ: 2k
        
        Ú©Ø§Ø±Ø¨Ø±Ø¯:
        --------
        - Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ³Ø· ØªØ§ Ø¨Ø²Ø±Ú¯ (n > 40)
        - Ø¨Ø±Ø§ÛŒ prediction Ø¨Ù‡ØªØ± Ø§Ø³Øª
        - Ù†Ø³Ø¨Øª Ø¨Ù‡ BICØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø±Ø§ ØªØ±Ø¬ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
        """
        return 2 * n_params - 2 * log_likelihood
    
    @staticmethod
    def compute_aic_c(log_likelihood: float, n_params: int, n_samples: int) -> float:
        """
        Corrected AIC (AICc) for small samples
        
        ÙØ±Ù…ÙˆÙ„: AICc = AIC + [2kÂ²+ 2k] / [n - k - 1]
        
        ØªÙˆØ¶ÛŒØ­:
        -------
        - Ø§ØµÙ„Ø§Ø­ AIC Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©
        - ÙˆÙ‚ØªÛŒ n/k < 40ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯
        - Ø¨Ø±Ø§ÛŒ n â†’ âˆ Ø¨Ù‡ AIC Ù…ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        
        Ú©Ø§Ø±Ø¨Ø±Ø¯:
        --------
        - Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© (n < 40)
        - Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting
        """
        aic = ModelSelection.compute_aic(log_likelihood, n_params)
        correction = (2 * n_params**2 + 2 * n_params) / (n_samples - n_params - 1)
        return aic + correction
    
    @staticmethod
    def compute_bic(log_likelihood: float, n_params: int, n_samples: int) -> float:
        """
        Bayesian Information Criterion (BIC)
        
        ÙØ±Ù…ÙˆÙ„: BIC = kÂ·ln(n) - 2ln(L)
        
        ØªÙˆØ¶ÛŒØ­:
        -------
        - Ø¬Ø±ÛŒÙ…Ù‡ Ù‚ÙˆÛŒâ€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ: kÂ·ln(n)
        - Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯
        - Ù…Ø¯Ù„ Ø¨Ø§ BIC Ú©Ù…ØªØ± Ø¨Ù‡ØªØ± Ø§Ø³Øª
        
        Ú©Ø§Ø±Ø¨Ø±Ø¯:
        --------
        - ÙˆÙ‚ØªÛŒ Ù‡Ø¯Ù identification Ù…Ø¯Ù„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª
        - Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø±Ø§ Ø¨ÛŒØ´ØªØ± ØªØ±Ø¬ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
        - Ø¨Ø±Ø§ÛŒ n Ø¨Ø²Ø±Ú¯ØŒ Ø¬Ø±ÛŒÙ…Ù‡ Ø´Ø¯ÛŒØ¯ØªØ± Ø§Ø² AIC
        
        ØªÙØ§ÙˆØª Ø¨Ø§ AIC:
        --------------
        - AIC: Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ prediction
        - BIC: Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ selection (Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¯Ø±Ø³Øª)
        """
        return n_params * np.log(n_samples) - 2 * log_likelihood
    
    @staticmethod
    def compute_likelihood(data: np.ndarray, distribution) -> float:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ log-likelihood
        
        ØªÙˆØ¶ÛŒØ­:
        -------
        - Ø§Ø­ØªÙ…Ø§Ù„ Ø¯ÛŒØ¯Ù† Ø¯Ø§Ø¯Ù‡ ØªØ­Øª Ù…Ø¯Ù„
        - log Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¨Ø±Ø§ÛŒ stability Ø¹Ø¯Ø¯ÛŒ
        """
        log_lik = np.sum(distribution.logpdf(data))
        return log_lik
    
    @staticmethod
    def compare_models(
        data: np.ndarray,
        fitted_distributions: List,
        criterion: str = 'aic'
    ) -> List[ModelScore]:
        """
        Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ø¨Ø§ ÛŒÚ© Ù…Ø¹ÛŒØ§Ø±
        
        Parameters:
        -----------
        data : array-like
            Ø¯Ø§Ø¯Ù‡
        fitted_distributions : list
            Ù„ÛŒØ³Øª ØªÙˆØ²ÛŒØ¹â€ŒÙ‡Ø§ÛŒ ÙÛŒØªâ€ŒØ´Ø¯Ù‡
        criterion : str
            'aic', 'aicc', 'bic', 'loo_cv'
        
        Returns:
        --------
        scores : list of ModelScore
            Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ù…Ø±ØªØ¨ Ø´Ø¯Ù‡ (Ø¨Ù‡ØªØ±ÛŒÙ† Ø§ÙˆÙ„)
        """
        n_samples = len(data)
        scores = []
        
        for dist in fitted_distributions:
            log_lik = ModelSelection.compute_likelihood(data, dist)
            n_params = len(dist.params)
            
            if criterion == 'aic':
                score = ModelSelection.compute_aic(log_lik, n_params)
                expl = ModelSelection._explain_aic(score, n_params, n_samples)
            elif criterion == 'aicc':
                score = ModelSelection.compute_aic_c(log_lik, n_params, n_samples)
                expl = ModelSelection._explain_aicc(score, n_params, n_samples)
            elif criterion == 'bic':
                score = ModelSelection.compute_bic(log_lik, n_params, n_samples)
                expl = ModelSelection._explain_bic(score, n_params, n_samples)
            elif criterion == 'loo_cv':
                score = ModelSelection.compute_loo_cv(data, dist)
                expl = ModelSelection._explain_loo(score)
            else:
                raise ValueError(f"Unknown criterion: {criterion}")
            
            scores.append(ModelScore(
                distribution_name=dist.info.name,
                criterion=criterion.upper(),
                score=score,
                n_params=n_params,
                sample_size=n_samples,
                explanation=expl
            ))
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ (Ú©Ù…ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² = Ø¨Ù‡ØªØ±ÛŒÙ†)
        scores.sort(key=lambda x: x.score)
        for rank, score_obj in enumerate(scores, 1):
            score_obj.rank = rank
        
        return scores
    
    @staticmethod
    def compute_loo_cv(data: np.ndarray, distribution) -> float:
        """
        Leave-One-Out Cross-Validation
        
        ØªÙˆØ¶ÛŒØ­:
        -------
        - Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù‚Ø·Ù‡ Ø¯Ø§Ø¯Ù‡:
          1. Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø¯ÙˆÙ† Ø¢Ù† Ù†Ù‚Ø·Ù‡ ÙÛŒØª Ú©Ù†
          2. log-likelihood Ø¢Ù† Ù†Ù‚Ø·Ù‡ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†
        - Ù…Ø¬Ù…ÙˆØ¹ log-likelihoods Ù…Ù†ÙÛŒ = LOO score
        
        Ù…Ø²Ø§ÛŒØ§:
        -------
        - Ù…Ø³ØªÙ‚ÛŒÙ… Ú©ÛŒÙÛŒØª prediction Ø±Ø§ Ù…ÛŒâ€ŒØ³Ù†Ø¬Ø¯
        - Ø¨Ù‡ overfitting Ø­Ø³Ø§Ø³ Ø§Ø³Øª
        - Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ø¯Ø§Ø±Ø¯
        
        Ù…Ø¹Ø§ÛŒØ¨:
        -------
        - Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ú¯Ø±Ø§Ù† (n Ø¨Ø§Ø± ÙÛŒØª)
        - Ø¨Ø±Ø§ÛŒ n Ø¨Ø²Ø±Ú¯ Ú©Ù†Ø¯ Ø§Ø³Øª
        """
        n = len(data)
        loo_scores = []
        
        for i in range(n):
            # Ø­Ø°Ù ÛŒÚ© Ù†Ù‚Ø·Ù‡
            train_data = np.delete(data, i)
            test_point = data[i:i+1]
            
            # ÙÛŒØª Ø±ÙˆÛŒ Ø¨Ù‚ÛŒÙ‡
            dist_temp = distribution.__class__()
            try:
                dist_temp.fit(train_data, method='mle')
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ log-likelihood Ù†Ù‚Ø·Ù‡â€ŒÛŒ Ø­Ø°Ùâ€ŒØ´Ø¯Ù‡
                log_lik = dist_temp.logpdf(test_point)[0]
                loo_scores.append(log_lik)
            except:
                # Ø§Ú¯Ø± ÙÛŒØª Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ØŒ Ø¬Ø±ÛŒÙ…Ù‡ Ø³Ù†Ú¯ÛŒÙ†
                loo_scores.append(-1e6)
        
        # Ù…Ù†ÙÛŒ Ù…Ø¬Ù…ÙˆØ¹ log-likelihoods
        return -np.sum(loo_scores)
    
    @staticmethod
    def _explain_aic(aic_value: float, n_params: int, n_samples: int) -> str:
        """ØªÙˆØ¶ÛŒØ­ AIC"""
        return f"""AIC = {aic_value:.2f}

ğŸ’¡ Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ø§Ø² Ø¯Ùˆ Ø¨Ø®Ø´ ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡:
   â€¢ Ø¬Ø±ÛŒÙ…Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ: 2Ã—{n_params} = {2*n_params}
   â€¢ Goodness of fit: -2Ã—log(likelihood)
   
ğŸ“Š ØªÙØ³ÛŒØ±:
   â€¢ Ø¹Ø¯Ø¯ Ú©ÙˆÚ†Ú©â€ŒØªØ± = Ù…Ø¯Ù„ Ø¨Ù‡ØªØ±
   â€¢ ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† fit Ø®ÙˆØ¨ Ùˆ Ø³Ø§Ø¯Ú¯ÛŒ
   â€¢ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ prediction
"""
    
    @staticmethod
    def _explain_aicc(aicc_value: float, n_params: int, n_samples: int) -> str:
        """ØªÙˆØ¶ÛŒØ­ AICc"""
        ratio = n_samples / n_params
        return f"""AICc = {aicc_value:.2f}

ğŸ’¡ Ø§ØµÙ„Ø§Ø­ AIC Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ú©ÙˆÚ†Ú©:
   â€¢ n/k = {ratio:.1f}
   â€¢ {"âš ï¸ Ù†Ù…ÙˆÙ†Ù‡ Ú©ÙˆÚ†Ú© - AICc Ø±Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†" if ratio < 40 else "âœ… Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø²Ø±Ú¯ - AIC Ú©Ø§ÙÛŒ Ø§Ø³Øª"}
"""
    
    @staticmethod
    def _explain_bic(bic_value: float, n_params: int, n_samples: int) -> str:
        """ØªÙˆØ¶ÛŒØ­ BIC"""
        penalty_ratio = np.log(n_samples) / 2
        return f"""BIC = {bic_value:.2f}

ğŸ’¡ Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ø¬Ø±ÛŒÙ…Ù‡ Ù‚ÙˆÛŒâ€ŒØªØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø¯Ø§Ø±Ø¯:
   â€¢ Ø¬Ø±ÛŒÙ…Ù‡: {n_params}Ã—ln({n_samples}) = {n_params * np.log(n_samples):.1f}
   â€¢ Ù†Ø³Ø¨Øª Ø¬Ø±ÛŒÙ…Ù‡ BIC/AIC: {penalty_ratio:.2f}Ã—
   
ğŸ“Š ØªÙØ³ÛŒØ±:
   â€¢ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø±Ø§ Ø¨ÛŒØ´ØªØ± ØªØ±Ø¬ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
   â€¢ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† "Ù…Ø¯Ù„ ÙˆØ§Ù‚Ø¹ÛŒ" Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª
   â€¢ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ nØŒ Ø¬Ø±ÛŒÙ…Ù‡ Ø´Ø¯ÛŒØ¯ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯
"""
    
    @staticmethod
    def _explain_loo(loo_value: float) -> str:
        """ØªÙˆØ¶ÛŒØ­ LOO-CV"""
        return f"""LOO-CV = {loo_value:.2f}

ğŸ’¡ Ø§Ù…ØªÛŒØ§Ø² cross-validation:
   â€¢ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ ØªÙˆØ§Ù† prediction Ø±Ø§ Ù…ÛŒâ€ŒØ³Ù†Ø¬Ø¯
   â€¢ Ù‡Ø± Ù†Ù‚Ø·Ù‡ ÛŒÚ©â€ŒØ¨Ø§Ø± test Ù…ÛŒâ€ŒØ´ÙˆØ¯
   â€¢ Ø¹Ø¯Ø¯ Ú©ÙˆÚ†Ú©â€ŒØªØ± = prediction Ø¨Ù‡ØªØ±
"""


class DeltaComparison:
    """
    Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Î” (delta) criteria
    
    Î”_i = criterion_i - criterion_best
    
    ØªÙØ³ÛŒØ±:
    -------
    - Î” < 2: Ù…Ø¯Ù„â€ŒÙ‡Ø§ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÛŒÚ©Ø³Ø§Ù†â€ŒØ§Ù†Ø¯
    - 2 < Î” < 7: Ù…Ø¯Ù„ Ø¨Ù‡ØªØ±ÛŒÙ† Ù‚Ø§Ø¨Ù„â€ŒØªÙˆØ¬Ù‡ Ø¨Ù‡ØªØ± Ø§Ø³Øª
    - Î” > 10: Ù…Ø¯Ù„ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ù‡â€ŒÙ…Ø±Ø§ØªØ¨ Ø¨Ù‡ØªØ± Ø§Ø³Øª
    """
    
    @staticmethod
    def compute_deltas(scores: List[ModelScore]) -> List[Dict]:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Î” Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„
        
        Returns:
        --------
        list of dict Ø¨Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª
        """
        best_score = scores[0].score  # Ú©Ù…ØªØ±ÛŒÙ†
        deltas = []
        
        for score in scores:
            delta = score.score - best_score
            interpretation = DeltaComparison._interpret_delta(delta)
            
            deltas.append({
                'model': score.distribution_name,
                'score': score.score,
                'delta': delta,
                'interpretation': interpretation
            })
        
        return deltas
    
    @staticmethod
    def _interpret_delta(delta: float) -> str:
        """ØªÙØ³ÛŒØ± Ù…Ù‚Ø¯Ø§Ø± Î”"""
        if delta < 2:
            return "âœ… ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÛŒÚ©Ø³Ø§Ù† Ø¨Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ - Ù‡Ø± Ø¯Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡"
        elif delta < 7:
            return "âš ï¸ Ù‚Ø§Ø¨Ù„â€ŒØªÙˆØ¬Ù‡ Ø¶Ø¹ÛŒÙâ€ŒØªØ± - Ø§Ú¯Ø± Ø¯Ù„ÛŒÙ„ Ø®Ø§ØµÛŒ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±Ø§ Ø¨Ú¯ÛŒØ±"
        else:
            return "âŒ Ø¨Ù‡â€ŒÙ…Ø±Ø§ØªØ¨ Ø¶Ø¹ÛŒÙâ€ŒØªØ± - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´ÙˆØ¯"
    
    @staticmethod
    def print_comparison(scores: List[ModelScore]):
        """Ú†Ø§Ù¾ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø²ÛŒØ¨Ø§"""
        deltas = DeltaComparison.compute_deltas(scores)
        
        print("\n" + "="*70)
        print("ğŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³", scores[0].criterion)
        print("="*70)
        print(f"{'Rank':<6} {'Model':<15} {'Score':<12} {'Î”':<10} {'ØªÙØ³ÛŒØ±'}")
        print("-"*70)
        
        for i, (score, delta_info) in enumerate(zip(scores, deltas), 1):
            print(f"{i:<6} {score.distribution_name:<15} "
                  f"{score.score:<12.2f} {delta_info['delta']:<10.2f} "
                  f"{delta_info['interpretation']}")
        
        print("="*70)
        print(f"\nğŸ† Ù…Ø¯Ù„ Ø¨Ø±ØªØ±: {scores[0].distribution_name}")
        print(f"\nğŸ’¡ ØªÙˆØ¶ÛŒØ­:")
        print(scores[0].explanation)
