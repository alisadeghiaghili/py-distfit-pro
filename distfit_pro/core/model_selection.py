"""\nModel Selection Criteria\n========================\n\nØ§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:\n- AIC (Akaike Information Criterion)\n- BIC (Bayesian Information Criterion)\n- WAIC (Watanabe-Akaike Information Criterion)\n- LOO-CV (Leave-One-Out Cross-Validation)\n\nÙ‡Ø± Ù…Ø¹ÛŒØ§Ø± ØªÙˆØ¶ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ú†Ø±Ø§ ÛŒÚ© Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ù‡ØªØ± Ù…ÛŒâ€ŒØ¯Ø§Ù†Ø¯.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport numpy as np\nfrom scipy import stats\n\n\n@dataclass\nclass ModelScore:\n    \"\"\"\n    Ø§Ù…ØªÛŒØ§Ø² ÛŒÚ© Ù…Ø¯Ù„ Ø¨Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª\n    \"\"\"\n    distribution_name: str\n    criterion: str\n    score: float\n    n_params: int\n    sample_size: int\n    explanation: str\n    rank: Optional[int] = None\n    \n    def __repr__(self) -> str:\n        return f\"{self.distribution_name}: {self.criterion}={self.score:.2f} (rank {self.rank})\"\n\n\nclass ModelSelection:\n    \"\"\"\n    Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„\n    \n    Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n    \"\"\"\n    \n    @staticmethod\n    def compute_aic(log_likelihood: float, n_params: int) -> float:\n        \"\"\"\n        Akaike Information Criterion (AIC)\n        \n        ÙØ±Ù…ÙˆÙ„: AIC = 2k - 2ln(L)\n        \n        ØªÙˆØ¶ÛŒØ­:\n        -------\n        - k: ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„\n        - L: likelihood\n        - Ù…Ø¯Ù„ Ø¨Ø§ AIC Ú©Ù…ØªØ± Ø¨Ù‡ØªØ± Ø§Ø³Øª\n        - Ø¬Ø±ÛŒÙ…Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ: 2k\n        \n        Ú©Ø§Ø±Ø¨Ø±Ø¯:\n        --------\n        - Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ³Ø· ØªØ§ Ø¨Ø²Ø±Ú¯ (n > 40)\n        - Ø¨Ø±Ø§ÛŒ prediction Ø¨Ù‡ØªØ± Ø§Ø³Øª\n        - Ù†Ø³Ø¨Øª Ø¨Ù‡ BICØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø±Ø§ ØªØ±Ø¬ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯\n        \"\"\"\n        return 2 * n_params - 2 * log_likelihood\n    \n    @staticmethod\n    def compute_aic_c(log_likelihood: float, n_params: int, n_samples: int) -> float:\n        \"\"\"\n        Corrected AIC (AICc) for small samples\n        \n        ÙØ±Ù…ÙˆÙ„: AICc = AIC + [2kÂ²+ 2k] / [n - k - 1]\n        \n        ØªÙˆØ¶ÛŒØ­:\n        -------\n        - Ø§ØµÙ„Ø§Ø­ AIC Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©\n        - ÙˆÙ‚ØªÛŒ n/k < 40ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯\n        - Ø¨Ø±Ø§ÛŒ n â†’ âˆ Ø¨Ù‡ AIC Ù…ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n        \n        Ú©Ø§Ø±Ø¨Ø±Ø¯:\n        --------\n        - Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© (n < 40)\n        - Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting\n        \"\"\"\n        aic = ModelSelection.compute_aic(log_likelihood, n_params)\n        correction = (2 * n_params**2 + 2 * n_params) / (n_samples - n_params - 1)\n        return aic + correction\n    \n    @staticmethod\n    def compute_bic(log_likelihood: float, n_params: int, n_samples: int) -> float:\n        \"\"\"\n        Bayesian Information Criterion (BIC)\n        \n        ÙØ±Ù…ÙˆÙ„: BIC = kÂ·ln(n) - 2ln(L)\n        \n        ØªÙˆØ¶ÛŒØ­:\n        -------\n        - Ø¬Ø±ÛŒÙ…Ù‡ Ù‚ÙˆÛŒâ€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ: kÂ·ln(n)\n        - Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯\n        - Ù…Ø¯Ù„ Ø¨Ø§ BIC Ú©Ù…ØªØ± Ø¨Ù‡ØªØ± Ø§Ø³Øª\n        \n        Ú©Ø§Ø±Ø¨Ø±Ø¯:\n        --------\n        - ÙˆÙ‚ØªÛŒ Ù‡Ø¯Ù identification Ù…Ø¯Ù„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª\n        - Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø±Ø§ Ø¨ÛŒØ´ØªØ± ØªØ±Ø¬ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯\n        - Ø¨Ø±Ø§ÛŒ n Ø¨Ø²Ø±Ú¯ØŒ Ø¬Ø±ÛŒÙ…Ù‡ Ø´Ø¯ÛŒØ¯ØªØ± Ø§Ø² AIC\n        \n        ØªÙØ§ÙˆØª Ø¨Ø§ AIC:\n        --------------\n        - AIC: Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ prediction\n        - BIC: Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ selection (Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¯Ø±Ø³Øª)\n        \"\"\"\n        return n_params * np.log(n_samples) - 2 * log_likelihood\n    \n    @staticmethod\n    def compute_likelihood(data: np.ndarray, distribution) -> float:\n        \"\"\"\n        Ù…Ø­Ø§Ø³Ø¨Ù‡ log-likelihood\n        \n        ØªÙˆØ¶ÛŒØ­:\n        -------\n        - Ø§Ø­ØªÙ…Ø§Ù„ Ø¯ÛŒØ¯Ù† Ø¯Ø§Ø¯Ù‡ ØªØ­Øª Ù…Ø¯Ù„\n        - log Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¨Ø±Ø§ÛŒ stability Ø¹Ø¯Ø¯ÛŒ\n        \"\"\"\n        log_lik = np.sum(distribution.logpdf(data))\n        return log_lik\n    \n    @staticmethod\n    def compare_models(\n        data: np.ndarray,\n        fitted_distributions: List,\n        criterion: str = 'aic'\n    ) -> List[ModelScore]:\n        \"\"\"\n        Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ø¨Ø§ ÛŒÚ© Ù…Ø¹ÛŒØ§Ø±\n        \n        Parameters:\n        -----------\n        data : array-like\n            Ø¯Ø§Ø¯Ù‡\n        fitted_distributions : list\n            Ù„ÛŒØ³Øª ØªÙˆØ²ÛŒØ¹â€ŒÙ‡Ø§ÛŒ ÙÛŒØªâ€ŒØ´Ø¯Ù‡\n        criterion : str\n            'aic', 'aicc', 'bic', 'loo_cv'\n        \n        Returns:\n        --------\n        scores : list of ModelScore\n            Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ù…Ø±ØªØ¨ Ø´Ø¯Ù‡ (Ø¨Ù‡ØªØ±ÛŒÙ† Ø§ÙˆÙ„)\n        \"\"\"\n        n_samples = len(data)\n        scores = []\n        \n        for dist in fitted_distributions:\n            log_lik = ModelSelection.compute_likelihood(data, dist)\n            n_params = len(dist.params)\n            \n            if criterion == 'aic':\n                score = ModelSelection.compute_aic(log_lik, n_params)\n                expl = ModelSelection._explain_aic(score, n_params, n_samples)\n            elif criterion == 'aicc':\n                score = ModelSelection.compute_aic_c(log_lik, n_params, n_samples)\n                expl = ModelSelection._explain_aicc(score, n_params, n_samples)\n            elif criterion == 'bic':\n                score = ModelSelection.compute_bic(log_lik, n_params, n_samples)\n                expl = ModelSelection._explain_bic(score, n_params, n_samples)\n            elif criterion == 'loo_cv':\n                score = ModelSelection.compute_loo_cv(data, dist)\n                expl = ModelSelection._explain_loo(score)\n            else:\n                raise ValueError(f\"Unknown criterion: {criterion}\")\n            \n            scores.append(ModelScore(\n                distribution_name=dist.info.name,\n                criterion=criterion.upper(),\n                score=score,\n                n_params=n_params,\n                sample_size=n_samples,\n                explanation=expl\n            ))\n        \n        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ (Ú©Ù…ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² = Ø¨Ù‡ØªØ±ÛŒÙ†)\n        scores.sort(key=lambda x: x.score)\n        for rank, score_obj in enumerate(scores, 1):\n            score_obj.rank = rank\n        \n        return scores\n    \n    @staticmethod\n    def compute_loo_cv(data: np.ndarray, distribution) -> float:\n        \"\"\"\n        Leave-One-Out Cross-Validation\n        \n        ØªÙˆØ¶ÛŒØ­:\n        -------\n        - Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù‚Ø·Ù‡ Ø¯Ø§Ø¯Ù‡:\n          1. Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø¯ÙˆÙ† Ø¢Ù† Ù†Ù‚Ø·Ù‡ ÙÛŒØª Ú©Ù†\n          2. log-likelihood Ø¢Ù† Ù†Ù‚Ø·Ù‡ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†\n        - Ù…Ø¬Ù…ÙˆØ¹ log-likelihoods Ù…Ù†ÙÛŒ = LOO score\n        \n        Ù…Ø²Ø§ÛŒØ§:\n        -------\n        - Ù…Ø³ØªÙ‚ÛŒÙ… Ú©ÛŒÙÛŒØª prediction Ø±Ø§ Ù…ÛŒâ€ŒØ³Ù†Ø¬Ø¯\n        - Ø¨Ù‡ overfitting Ø­Ø³Ø§Ø³ Ø§Ø³Øª\n        - Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ø¯Ø§Ø±Ø¯\n        \n        Ù…Ø¹Ø§ÛŒØ¨:\n        -------\n        - Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ú¯Ø±Ø§Ù† (n Ø¨Ø§Ø± ÙÛŒØª)\n        - Ø¨Ø±Ø§ÛŒ n Ø¨Ø²Ø±Ú¯ Ú©Ù†Ø¯ Ø§Ø³Øª\n        \"\"\"\n        n = len(data)\n        loo_scores = []\n        \n        for i in range(n):\n            # Ø­Ø°Ù ÛŒÚ© Ù†Ù‚Ø·Ù‡\n            train_data = np.delete(data, i)\n            test_point = data[i:i+1]\n            \n            # ÙÛŒØª Ø±ÙˆÛŒ Ø¨Ù‚ÛŒÙ‡\n            dist_temp = distribution.__class__()\n            try:\n                dist_temp.fit(train_data, method='mle')\n                # Ù…Ø­Ø§Ø³Ø¨Ù‡ log-likelihood Ù†Ù‚Ø·Ù‡â€ŒÛŒ Ø­Ø°Ùâ€ŒØ´Ø¯Ù‡\n                log_lik = dist_temp.logpdf(test_point)[0]\n                loo_scores.append(log_lik)\n            except:\n                # Ø§Ú¯Ø± ÙÛŒØª Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ØŒ Ø¬Ø±ÛŒÙ…Ù‡ Ø³Ù†Ú¯ÛŒÙ†\n                loo_scores.append(-1e6)\n        \n        # Ù…Ù†ÙÛŒ Ù…Ø¬Ù…ÙˆØ¹ log-likelihoods\n        return -np.sum(loo_scores)\n    \n    @staticmethod\n    def _explain_aic(aic_value: float, n_params: int, n_samples: int) -> str:\n        \"\"\"ØªÙˆØ¶ÛŒØ­ AIC\"\"\"\n        return f\"\"\"\nAIC = {aic_value:.2f}\n\nğŸ’¡ Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ø§Ø² Ø¯Ùˆ Ø¨Ø®Ø´ ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡:\n   â€¢ Ø¬Ø±ÛŒÙ…Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ: 2Ã—{n_params} = {2*n_params}\n   â€¢ Goodness of fit: -2Ã—log(likelihood)\n   \nğŸ“Š ØªÙØ³ÛŒØ±:\n   â€¢ Ø¹Ø¯Ø¯ Ú©ÙˆÚ†Ú©â€ŒØªØ± = Ù…Ø¯Ù„ Ø¨Ù‡ØªØ±\n   â€¢ ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† fit Ø®ÙˆØ¨ Ùˆ Ø³Ø§Ø¯Ú¯ÛŒ\n   â€¢ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ prediction\n\"\"\"\n    \n    @staticmethod\n    def _explain_aicc(aicc_value: float, n_params: int, n_samples: int) -> str:\n        \"\"\"ØªÙˆØ¶ÛŒØ­ AICc\"\"\"\n        ratio = n_samples / n_params\n        return f\"\"\"\nAICc = {aicc_value:.2f}\n\nğŸ’¡ Ø§ØµÙ„Ø§Ø­ AIC Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ú©ÙˆÚ†Ú©:\n   â€¢ n/k = {ratio:.1f}\n   â€¢ {\"âš ï¸ Ù†Ù…ÙˆÙ†Ù‡ Ú©ÙˆÚ†Ú© - AICc Ø±Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†\" if ratio < 40 else \"âœ… Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø²Ø±Ú¯ - AIC Ú©Ø§ÙÛŒ Ø§Ø³Øª\"}\n\"\"\"\n    \n    @staticmethod\n    def _explain_bic(bic_value: float, n_params: int, n_samples: int) -> str:\n        \"\"\"ØªÙˆØ¶ÛŒØ­ BIC\"\"\"\n        penalty_ratio = np.log(n_samples) / 2\n        return f\"\"\"\nBIC = {bic_value:.2f}\n\nğŸ’¡ Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ø¬Ø±ÛŒÙ…Ù‡ Ù‚ÙˆÛŒâ€ŒØªØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø¯Ø§Ø±Ø¯:\n   â€¢ Ø¬Ø±ÛŒÙ…Ù‡: {n_params}Ã—ln({n_samples}) = {n_params * np.log(n_samples):.1f}\n   â€¢ Ù†Ø³Ø¨Øª Ø¬Ø±ÛŒÙ…Ù‡ BIC/AIC: {penalty_ratio:.2f}Ã—\n   \nğŸ“Š ØªÙØ³ÛŒØ±:\n   â€¢ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø±Ø§ Ø¨ÛŒØ´ØªØ± ØªØ±Ø¬ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯\n   â€¢ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† \"Ù…Ø¯Ù„ ÙˆØ§Ù‚Ø¹ÛŒ\" Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª\n   â€¢ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ nØŒ Ø¬Ø±ÛŒÙ…Ù‡ Ø´Ø¯ÛŒØ¯ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯\n\"\"\"\n    \n    @staticmethod\n    def _explain_loo(loo_value: float) -> str:\n        \"\"\"ØªÙˆØ¶ÛŒØ­ LOO-CV\"\"\"\n        return f\"\"\"\nLOO-CV = {loo_value:.2f}\n\nğŸ’¡ Ø§Ù…ØªÛŒØ§Ø² cross-validation:\n   â€¢ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ ØªÙˆØ§Ù† prediction Ø±Ø§ Ù…ÛŒâ€ŒØ³Ù†Ø¬Ø¯\n   â€¢ Ù‡Ø± Ù†Ù‚Ø·Ù‡ ÛŒÚ©â€ŒØ¨Ø§Ø± test Ù…ÛŒâ€ŒØ´ÙˆØ¯\n   â€¢ Ø¹Ø¯Ø¯ Ú©ÙˆÚ†Ú©â€ŒØªØ± = prediction Ø¨Ù‡ØªØ±\n\"\"\"\n\n\nclass DeltaComparison:\n    \"\"\"\n    Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Î” (delta) criteria\n    \n    Î”_i = criterion_i - criterion_best\n    \n    ØªÙØ³ÛŒØ±:\n    -------\n    - Î” < 2: Ù…Ø¯Ù„â€ŒÙ‡Ø§ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÛŒÚ©Ø³Ø§Ù†â€ŒØ§Ù†Ø¯\n    - 2 < Î” < 7: Ù…Ø¯Ù„ Ø¨Ù‡ØªØ±ÛŒÙ† Ù‚Ø§Ø¨Ù„â€ŒØªÙˆØ¬Ù‡ Ø¨Ù‡ØªØ± Ø§Ø³Øª\n    - Î” > 10: Ù…Ø¯Ù„ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ù‡â€ŒÙ…Ø±Ø§ØªØ¨ Ø¨Ù‡ØªØ± Ø§Ø³Øª\n    \"\"\"\n    \n    @staticmethod\n    def compute_deltas(scores: List[ModelScore]) -> List[Dict]:\n        \"\"\"\n        Ù…Ø­Ø§Ø³Ø¨Ù‡ Î” Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n        \n        Returns:\n        --------\n        list of dict Ø¨Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª\n        \"\"\"\n        best_score = scores[0].score  # Ú©Ù…ØªØ±ÛŒÙ†\n        deltas = []\n        \n        for score in scores:\n            delta = score.score - best_score\n            interpretation = DeltaComparison._interpret_delta(delta)\n            \n            deltas.append({\n                'model': score.distribution_name,\n                'score': score.score,\n                'delta': delta,\n                'interpretation': interpretation\n            })\n        \n        return deltas\n    \n    @staticmethod\n    def _interpret_delta(delta: float) -> str:\n        \"\"\"ØªÙØ³ÛŒØ± Ù…Ù‚Ø¯Ø§Ø± Î”\"\"\"\n        if delta < 2:\n            return \"âœ… ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÛŒÚ©Ø³Ø§Ù† Ø¨Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ - Ù‡Ø± Ø¯Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡\"\n        elif delta < 7:\n            return \"âš ï¸ Ù‚Ø§Ø¨Ù„â€ŒØªÙˆØ¬Ù‡ Ø¶Ø¹ÛŒÙâ€ŒØªØ± - Ø§Ú¯Ø± Ø¯Ù„ÛŒÙ„ Ø®Ø§ØµÛŒ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±Ø§ Ø¨Ú¯ÛŒØ±\"\n        else:\n            return \"âŒ Ø¨Ù‡â€ŒÙ…Ø±Ø§ØªØ¨ Ø¶Ø¹ÛŒÙâ€ŒØªØ± - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´ÙˆØ¯\"\n    \n    @staticmethod\n    def print_comparison(scores: List[ModelScore]):\n        \"\"\"Ú†Ø§Ù¾ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø²ÛŒØ¨Ø§\"\"\"\n        deltas = DeltaComparison.compute_deltas(scores)\n        \n        print(\"\\n\" + \"=\"*70)\n        print(\"ğŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³\", scores[0].criterion)\n        print(\"=\"*70)\n        print(f\"{'Rank':<6} {'Model':<15} {'Score':<12} {'Î”':<10} {'ØªÙØ³ÛŒØ±'}\")\n        print(\"-\"*70)\n        \n        for i, (score, delta_info) in enumerate(zip(scores, deltas), 1):\n            print(f\"{i:<6} {score.distribution_name:<15} \"\n                  f\"{score.score:<12.2f} {delta_info['delta']:<10.2f} \"\n                  f\"{delta_info['interpretation']}\")\n        \n        print(\"=\"*70)\n        print(f\"\\nğŸ† Ù…Ø¯Ù„ Ø¨Ø±ØªØ±: {scores[0].distribution_name}\")\n        print(f\"\\nğŸ’¡ ØªÙˆØ¶ÛŒØ­:\")\n        print(scores[0].explanation)\n